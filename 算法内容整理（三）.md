# 算法内容整理（三）

​	这一节主要是关于概率之间的知识。



## 最大似然估计和最大后验概率

​	在概率论和统计学中，*P*(*x*∣*θ*) 表示在给定参数 *θ* 的情况下，观测数据 *x* 发生的概率。在这个条件概率中：

- *x* 代表观测数据或样本，是已知的；
- *θ* 代表参数，可能是模型中的一些未知参数。

当 *x* 不确定时，表示我们对观测数据的取值不确定，这通常用于推断或预测。而当 *θ* 不确定时，表示我们对模型中的参数取值不确定，这通常用于参数估计和模型选择。

- 当 *x* 不确定时，我们可能希望通过观察现有的数据来对未知的 *x* 进行推断，例如通过条件概率 *P*(*x*∣*θ*) 来计算给定参数 *θ* 下观测数据 *x* 的可能性。
- 当 *θ* 不确定时，我们可能希望根据观测数据来估计参数的取值，例如通过最大似然估计或贝叶斯推断来找到最可能的参数值。

因此，*P*(*x*∣*θ*) 提供了在给定参数的情况下观测数据发生的概率，而不确定性主要来自于我们对观测数据或模型参数的不确定性。



### 最大似然估计

​	最大似然估计是一种常用的参数估计方法，其思想是寻找使观测数据出现的可能性最大的参数值。给定观测数据 *x* 和模型参数 *θ*，最大似然估计就是找到使得观测数据 *x* 的概率 *P*(*x*∣*θ*) 最大的参数值$\hat{\theta}$，即： 
$$
\hat{\theta}_{MLE} = argmax_\theta P(x|\theta)
$$
通常，为了便于计算，我们会对概率的对数取负数，将最大化问题转化为最小化问题。

​	

### 最大后验概率估计

​	最大后验概率估计考虑了先验概率和似然函数，并在这两者的基础上进行参数估计。给定观测数据 *x*，我们不仅考虑了观测数据的概率 *P*(*x*∣*θ*)，还引入了参数的先验分布 *P*(*θ*)，得到了后验概率 *P*(*θ*∣*x*)。最大后验概率估计就是找到使后验概率 *P*(*θ*∣*x*) 最大的参数值 $\hat{\theta}$，即：
$$
\hat{\theta}_{MAP} = argmax_\theta P(\theta | x)
$$
​	通常使用贝叶斯定理来表示后验概率：
$$
P(\theta | x) = \frac{P(x|\theta)·(\theta)}{P(x)}
$$
​	其中，$P(\theta | D)$是给定观测数据D条件下参数$\theta$的后验概率，$P(x|\theta)$是似然函数，$P(\theta)$是参数的先验分布，$P(x)$是数据的边际概率。

​	最大似然估计和最大后验概率估计在某种程度上都是寻找最可能的参数值，但最大后验概率估计引入了参数的先验信息，可以在数据不充分或不足时提供更稳健的估计结果。

【边际概率是在概率论和统计学中的一个概念，用于描述一个事件在给定条件下的概率。简单来说，边际概率指的是在某个事件发生的情况下，另一个事件发生的概率。通常情况下，边际概率是通过对联合概率分布进行边际化得到的。

例如，假设有两个随机变量 X 和 Y，它们之间存在某种关联。那么给定 X 的取值，Y 发生的概率就是边际概率。用数学符号表示为 P(Y|X)。】



## 各种分布【泊松分布等等】

https://blog.csdn.net/Ga4ra/article/details/78935537

### 0-1分布的知识补充

问题：0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器

要将服从于0到1均匀分布的随机变量转换为均值为0，方差为1的随机变量，可以使用以下步骤：

1. 将0到1均匀分布的随机变量记为 X。
2. 计算 X 的均值和方差。由于均匀分布的均值为 (a + b) / 2，方差为 ((b - a)^2^) / 12，其中 a 和 b 分别为分布的下限和上限。对于0到1均匀分布，均值为 0.5，方差为 1/12。
3. 将 X 减去其均值，并除以其标准差（方差的平方根）。这样可以使得新的随机变量 Y 具有均值为0，方差为1。

具体地，将 X 变换为 Y 的公式为：
$$
Y = \frac{{X - 0.5}}{{\sqrt{\frac{1}{12}}}}
$$
通过这个变换，得到的 Y 就是均值为0，方差为1的随机变量。



## 期望、方差和偏差

https://www.cnblogs.com/makefile/p/bias-var.html

这里再做一次总结，先说一个每个符号代表的意思：

$D$  数据集

$y_D$ $x$在数据集中的标记，如果是推荐的话其实很少有这个

$y$ $x$的真实标记

$f$ 训练集$D$学得的模型

$f(x;D),\quad P(x)$ 有训练集$D$学得的模型$f$对$x$的预测输出(P（x）是指离散)

$\overline{f}(x)$ 模型$f$对$x$的期望预测输出

### 误差

$$
Err(x) = E[(y-f(x;D))^2]
$$

​	这是在连续变量里面的误差。

### 方差

方差实际上就是实际的预测值减去期望预测的期望

如果说一个训练集$D$上模型$f$对测试样本$x$的预测输出为$f(x;D)$，那么学习算法$f$对测试样本$x$的期望预测为：
$$
\overline{f}(x) = E_D[f(x;D)]
$$
那么方差就可以写作：
$$
var(x) = E_D[(f(x;D)-\overline{f}(x))^2]
$$

### 偏差

期望预测与真实标记的误差称之为偏差：
$$
bias^2(x) = (\overline{f}(x)-y)^2
$$
偏差是模型预测值与真实值之间的差异，表示了模型的预测是否偏向于某个方向。在深度学习中，偏差通常用来描述模型的拟合能力，即模型对于数据的整体趋势的拟合程度。