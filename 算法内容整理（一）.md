# 算法内容整理（一）

## softmax相关知识

### softmax定义和基础

​	Softmax 是一种常用的归一化函数，通常用于多类别分类问题中，它可以将一组数值转换为表示概率分布的数值。

​	Softmax 函数的公式如下：

​	给定输入向量x = (x~1~, x~2~, x~3~, ..., x~n~)，Softmax 函数将输入向量转换为概率分布 p = (p~1~, p~2~, ..., p~n~)，其中p~i~表示第i个元素的概率，满足以下公式：
$$
p_i = \frac{e^{x_i}}{\sum^n_{j=1}e^{x_j}}
$$

### softmax的数值上溢

​	在计算 Softmax 函数时，如果输入向量 \( x \) 中的某些元素具有较大的值，那么其指数 \( e^{x_i} \) 可能会非常大，这可能导致计算时数值上溢（overflow）。

​	· 数值上溢是指在计算机中表示的数字超出了其可以表示的范围，导致结果变为无穷大或者 NaN（不是一个数字）。

​	为了避免数值上溢，通常会对输入向量 \( x \) 进行一些调整，例如，可以在计算 Softmax 函数之前，从每个元素中减去该向量中的最大值。这个操作不会改变 Softmax 函数的输出结果，但它可以确保输入向量的值相对较小，从而避免了指数函数的溢出问题。这种处理方式称为 Softmax 的数值稳定性处理（numerical stability），它保证了 Softmax 函数在计算机中的可靠性和稳定性。它的过程为：
 	1. 找到输入向量中的最大值max_value， 然后对每个元素x~i~执行以下操作：
​     x~i~ = x~i~ - max_value

​	除此之外，如果你使用的是支持大数运算的数值计算库，例如 NumPy 或 TensorFlow，在计算 Softmax 函数时，它们通常会内置数值稳定性处理的功能，以防止指数上溢。因此，使用这些库中的 Softmax 函数可能会更安全。



## 特征相关技术

- PCA
- 拟牛顿法
- 核函数相关内容
- DBSCAN

### PCA

PCA 是主成分分析（Principal Component Analysis）的缩写，是一种常用的数据降维技术和特征提取方法。它通过线性变换将原始数据投影到一个新的特征空间中，使得数据在新的特征空间中的方差最大化。这些新特征被称为主成分，它们是原始特征的线性组合。

主成分分析的步骤如下：

1. 对原始数据进行标准化处理，使得每个特征的均值为0，方差为1（或者按照需求进行缩放）。
2. 计算数据的协方差矩阵。
3. 对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。
4. 选择前 k 个特征值对应的特征向量，这些特征向量就是主成分。
5. 通过将原始数据投影到选定的主成分上，即可得到降维后的数据。

PCA 可以应用于数据压缩、特征提取、可视化、噪声过滤等领域。它在数据探索、数据可视化和预处理中都有广泛的应用。

一个实际的 PCA 的例子是人脸识别。在人脸识别任务中，图像通常具有非常高的维度，每个像素都可以看作是一个特征。然而，这些高维度的特征可能会导致计算复杂度高和数据稀疏的问题。

通过 PCA，可以将这些高维度的人脸图像转换为一个更低维度的特征空间，同时保留图像的主要特征。具体步骤如下：

1. 收集大量的人脸图像数据，并将它们转换成矩阵形式，其中每一行代表一个图像，每一列代表一个像素。
2. 对这些图像数据进行标准化处理，使得每个像素的均值为0，方差为1。
3. 计算协方差矩阵。
4. 对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。
5. 选择前 k 个特征值对应的特征向量，这些特征向量就是主成分。
6. 通过将原始图像数据投影到选定的主成分上，即可得到降维后的特征向量。
7. 使用降维后的特征向量进行人脸识别任务，例如使用支持向量机（SVM）或者神经网络等分类器。

通过 PCA 降维，可以减少计算量和存储空间，并且提高分类器的性能，同时保留了图像的主要特征，从而实现了高效的人脸识别系统。



### 拟牛顿法

​	拟牛顿法是一种数值优化算法，用于解决无约束或约束的非线性优化问题。它是基于牛顿法的一种改进，通过逼近牛顿法中计算复杂的 Hessian 矩阵（二阶导数矩阵）的逆来加速收敛。

​	在拟牛顿法中，不直接计算 Hessian 矩阵的逆，而是通过构建一个近似的 Hessian 矩阵的逆来代替。这个近似矩阵通常被称为拟 Hessian 矩阵。拟牛顿法的主要思想是根据当前和之前的迭代点的梯度值和函数值来不断更新拟 Hessian 矩阵的逆，从而逼近真实的 Hessian 矩阵的逆。

​	一个常见的拟牛顿法是 BFGS 算法（Broyden-Fletcher-Goldfarb-Shanno 算法），它通过迭代地更新一个近似的 Hessian 矩阵的逆来优化目标函数。

​	举个例子，假设我们要最小化一个函数 f(x)，其中 x 是一个向量。我们可以使用拟牛顿法来找到使得函数 f(x) 最小化的 x。在每次迭代中，我们计算当前点的梯度，然后利用这个梯度信息更新拟 Hessian 矩阵的逆，然后利用更新后的逆矩阵来更新下一个迭代点的位置。这样不断迭代直到满足停止条件为止，从而找到函数 f(x) 的最小值点 x。



### 核函数

​	核函数是支持向量机（SVM）等机器学习算法中的一种技巧，用于将原始输入空间中的数据映射到一个更高维的特征空间，以便在特征空间中更容易地进行线性分类或回归。核函数允许在不显式计算映射到高维空间的特征向量的情况下，在原始输入空间中计算出特征空间中的内积，从而避免了显式地计算高维空间的特征向量，节省了计算成本。

核函数的种类有很多，常见的核函数包括：

1. **线性核函数**（Linear Kernel）：
   $$
   K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j
   $$
   线性核函数适用于线性可分的情况，可以直接在原始输入空间中进行线性分类。
   
2. **多项式核函数**（Polynomial Kernel）：
   $$
   K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^T \mathbf{x}_j + c)^d
   $$
   多项式核函数将数据映射到更高维的空间，并引入多项式的非线性特征，适用于一些非线性问题。
   
3. **高斯核函数**（Gaussian Kernel，也称为径向基函数 RBF）：
   $$
   K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma^2}\right)
   $$
   高斯核函数通过计算输入样本之间的相似度，将数据映射到无穷维的特征空间，适用于复杂的非线性分类问题。
   
4. **Sigmoid核函数**（Sigmoid Kernel）：
   $$
   K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\alpha \mathbf{x}_i^T \mathbf{x}_j + c) 
   $$
   Sigmoid核函数可以实现类似神经网络中的激活函数的效果，适用于一些特定的问题。
   
5. **其他核函数**：
   还有一些其他的核函数，如指数核函数、拉普拉斯核函数等，适用于不同的场景和问题。

核函数的应用场景包括但不限于：
- 非线性分类问题：在原始输入空间中无法线性可分的情况下，使用核函数将数据映射到更高维的特征空间，从而实现非线性分类。
- 文本分类：在文本分类等自然语言处理任务中，使用核函数进行词频特征的映射，以便进行分类。
- 图像识别：在图像识别等计算机视觉任务中，使用核函数将图像数据映射到高维特征空间，以便进行分类或者检测。
- 数据降维：在主成分分析（PCA）等降维方法中，使用核函数将数据映射到高维空间，以便进行降维分析。



### FM、FFM和DeepFM

https://blog.csdn.net/hiwallace/article/details/81333604?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-81333604-blog-79998959.235%5Ev43%5Epc_blog_bottom_relevance_base2&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-81333604-blog-79998959.235%5Ev43%5Epc_blog_bottom_relevance_base2&utm_relevant_index=1



## 距离计算

在机器学习中，常用的距离计算方法包括：

1. **欧氏距离（Euclidean Distance）**：
   - 欧氏距离是最常见的距离度量方法，用于计算两个样本之间在特征空间中的直线距离。
   
   - 公式为：
     $$
     d(\mathbf{p}, \mathbf{q}) = \sqrt{\sum_{i=1}^{n}(p_i - q_i)^2}
     $$
     
   - 其中，$\mathbf{p}$ 和$\mathbf{q}$是两个样本的特征向量，$p_i$ 和 $q_i$ 分别是两个样本在第 i个特征上的取值。
   
2. **曼哈顿距离（Manhattan Distance）**：
   - 曼哈顿距离也称为城市街区距离，用于计算两个样本在特征空间中的沿坐标轴的距离之和。
   - 公式为：

   $$
   d(\mathbf{p}, \mathbf{q}) = \sum_{i=1}^{n} |p_i - q_i
   $$
   
   
   
3. **闵可夫斯基距离（Minkowski Distance）**：
   - 闵可夫斯基距离是欧氏距离和曼哈顿距离的一般化形式，可以通过调节参数p来控制距离的计算方式。
   - 当 \( p = 2 \) 时，闵可夫斯基距离退化为欧氏距离；当 \( p = 1 \) 时，闵可夫斯基距离退化为曼哈顿距离。

4. **切比雪夫距离（Chebyshev Distance）**：
   
   - 切比雪夫距离是计算两个样本之间的最大绝对值差距。
   - 公式为：
   
   $$
   d(\mathbf{p}, \mathbf{q}) = \max_{i=1}^{n} |p_i - q_i|
   $$
   
   
   
5. **余弦相似度（Cosine Similarity）**：
   
   - 余弦相似度用于衡量两个样本之间的方向相似程度，而不是直接的距离。
   - 公式为：
   
   $$
   \text{similarity}(\mathbf{p}, \mathbf{q}) = \frac{\mathbf{p} \cdot \mathbf{q}}{\|\mathbf{p}\| \|\mathbf{q}\|}
   $$
   
   
   
   - 其中，$\mathbf{p} \cdot \mathbf{q}$是两个样本的内积，$||\mathbf{p}||$和$||\mathbf{q}||$分别是两个样本的范数。

以上是常用的距离计算方法，选择合适的距离度量方法需要根据具体的问题和数据特征进行考虑。



## 生成模型和判别模型

- 生成模型

​	生成模型试图建模并学习数据的生成过程，即学习联合概率分布 *P*(*X*,*Y*)，其中 *X* 是输入特征（观测数据），*Y* 是输出标签或类别。生成模型可以用来模拟从给定输入到输出的转换过程。生成模型的基本形式可以表示为： *P*(*X*,*Y*)=*P*(*Y*∣*X*)*P*(*X*) 其中，*P*(*X*) 是输入数据的分布，*P*(*Y*∣*X*) 是在给定输入数据 *X* 的条件下输出标签 *Y* 的条件分布。

- 判别模型

​	判别模型则直接学习并建模类别之间的决策边界或者条件概率分布 *P*(*Y*∣*X*)，即学习条件概率分布 *P*(*Y*∣*X*) 或者直接学习决策函数 *f*(*X*)。判别模型的基本形式可以表示为： *P*(*Y*∣*X*) 或者 *f*(*X*)

​	生成模型和判别模型之间的区别在于，生成模型学习的是联合概率分布，它能够模拟从输入到输出的整个生成过程；而判别模型学习的是条件概率分布或者直接学习决策函数，它关注的是给定输入的条件下输出的分布或者类别。因此，生成模型更多地关注数据的分布和生成规律，而判别模型更多地关注类别之间的边界和决策过程。

​	常见的生成模型包括朴素贝叶斯（Naive Bayes）、高斯混合模型（Gaussian Mixture Model，GMM）、隐马尔可夫模型（Hidden Markov Model，HMM）等；常见的判别模型包括逻辑回归（Logistic Regression）、支持向量机（Support Vector Machine，SVM）、决策树（Decision Tree）、神经网络（Neural Network）等。



## 聚类

### DBSCAN

DBSCAN是一种密度聚类算法，全称为Density-Based Spatial Clustering of Applications with Noise。它是一种非参数化的聚类算法，能够识别具有高密度的区域，并且能够将这些区域划分为单个聚类。

DBSCAN的核心思想是基于密度的聚类，它不需要预先指定聚类的数量，而是通过在数据集中寻找密度相连的点来识别聚类。在DBSCAN中，每个数据点都被标记为核心点、边界点或噪声点。

DBSCAN算法的主要参数包括两个：eps（ϵ）和minPts。其中，eps（ϵ）表示邻域半径，用于确定一个点的邻域范围；minPts表示一个点的邻域中至少包含的点的数量，用于确定核心点。通过调节这两个参数，可以控制聚类的密度和形状。

DBSCAN算法的主要步骤如下：
1. 选择一个未被访问的数据点，并判断其邻域内的点的数量是否大于等于minPts。如果大于等于minPts，则将该点标记为核心点，并将其邻域内的所有点加入当前聚类。
2. 遍历核心点的邻域内的所有点，将其中未被访问的点加入当前聚类。
3. 重复步骤1和步骤2，直到当前聚类中的所有点都被访问。
4. 选择下一个未被访问的数据点，并重复步骤1至步骤3，直到所有数据点都被访问。

DBSCAN算法的优势在于它能够自动识别任意形状的聚类，并且能够有效地处理噪声数据。然而，DBSCAN也有一些缺点，例如对eps（ϵ）和minPts参数的选择较为敏感，以及在处理高维数据时可能存在维度灾难问题。



### OPTICS

OPTICS（Ordering Points To Identify the Clustering Structure）算法是一种用于聚类的密度可达的基于密度的聚类算法，它可以发现具有不同密度的聚类结构，并且不需要事先指定聚类数量。OPTICS 算法是 DBSCAN（Density-Based Spatial Clustering of Applications with Noise）算法的扩展，它通过构建对象之间的可达性图来识别聚类结构。

以下是 OPTICS 算法的基本步骤：

1. **距离计算**：计算每对数据点之间的距离，并根据事先指定的邻域半径参数（ε）确定每个数据点的 ε-邻域。

2. **核心距离计算**：对于每个数据点，计算其 ε-邻域内的最小距离，称为核心距离（core distance）。核心距离是一个指示数据点密度的度量，较小的核心距离表示数据点周围的密度较高。

3. **可达距离计算**：对于每个数据点，计算其到相邻数据点的可达距离（reachability distance）。可达距离是一个指示数据点之间关系的度量，表示从当前数据点出发，沿着密度可达路径到达相邻数据点的距离，取较大值作为可达距离。

4. **对象排序**：根据每个数据点的核心距离和可达距离，对数据点进行排序。排序后，数据点按照密度从高到低的顺序排列，密度较高的数据点在前面，密度较低的数据点在后面。

5. **聚类识别**：通过遍历排序后的数据点，根据每个数据点的可达距离，识别聚类结构。具体地，对于每个数据点，如果其可达距离超过事先指定的阈值（MinPts），则将其标记为一个新的聚类中心；如果其可达距离未超过阈值，则将其归属到与其最近的密度较高的数据点所在的聚类中。

通过上述步骤，OPTICS 算法可以发现具有不同密度的聚类结构，并且不需要预先指定聚类数量。相较于 DBSCAN 算法，OPTICS 算法的主要优势在于能够识别不同密度的聚类结构，并且不受邻域半径参数的选择影响。



### 层次聚类

层次聚类是一种基于树结构的聚类方法，它将数据点逐步合并成聚类，形成一个聚类树（或者叫做聚类谱）。层次聚类可以分为两种主要的方式：凝聚型层次聚类（Agglomerative Hierarchical Clustering）和分裂型层次聚类（Divisive Hierarchical Clustering）。

1. **凝聚型层次聚类**：
   在凝聚型层次聚类中，算法开始时将每个数据点视为一个单独的聚类，然后逐步合并最相似的聚类，直到所有的数据点都被合并成一个聚类为止。这个过程可以用一个树状结构（树状图或者树状图谱）来表示，树的根节点表示所有数据点的一个大聚类，叶子节点表示单个数据点的聚类。凝聚型层次聚类的主要步骤包括计算距离（或者相似度）矩阵、合并最相似的聚类、更新距离矩阵，直到所有数据点都被合并成一个聚类。

2. **分裂型层次聚类**：
   在分裂型层次聚类中，算法开始时将所有的数据点视为一个大聚类，然后逐步地将大聚类分裂成更小的聚类，直到每个聚类只包含一个数据点为止。分裂型层次聚类的主要思想是不断地将具有较大差异的数据点分开，直到每个数据点都被分配到一个单独的聚类中。分裂型层次聚类的主要步骤包括计算距离（或者相似度）矩阵、选择最不相似的聚类、将选择的聚类分裂成两个较小的聚类，重复这个过程直到每个聚类只包含一个数据点。

凝聚型层次聚类和分裂型层次聚类各有其优点和缺点，选择哪种方法取决于数据的特点以及聚类的目标。通常来说，凝聚型层次聚类比较常用，因为它的实现相对简单，而且能够生成一个完整的聚类树，可以方便地根据需要进行聚类结果的截断。



## 集成学习方法

Bagging（Bootstrap Aggregating）和Boosting 是两种常见的集成学习方法，它们都可以提高模型的性能，但在原理和操作上有一些区别：

1. **Bagging（自举聚合）**：
   - Bagging 是一种并行的集成学习方法，它通过对原始数据集进行有放回抽样（bootstrap sampling），产生多个不同的训练数据集，然后基于这些数据集训练多个基学习器。
   - 每个基学习器都是相互独立地训练的，没有先后顺序，可以同时进行。最后，通过对多个基学习器的预测结果进行投票或者取平均值，来得到最终的集成模型。
   - 常见的 Bagging 方法包括随机森林（Random Forest）和 Bagged Decision Trees 等。
2. **Boosting（提升）**：
   - Boosting 是一种串行的集成学习方法，它通过迭代训练多个基学习器，每次训练都会根据前一次训练的结果来调整数据集的权重，使得在前一次训练中被错误分类的样本在下一次训练中得到更多的关注。
   - 每个基学习器都是基于前一个基学习器的残差（residual）进行训练的，以便逐步改进模型的性能。
   - Boosting 方法通常会在每次迭代过程中，根据模型在训练集上的表现来调整样本权重，使得模型能够重点关注那些在前一次迭代中被错误分类的样本。常见的 Boosting 方法包括 AdaBoost、Gradient Boosting 和 XGBoost 等。
3. **Stacking**：

​	Stacking 的基本思想是将多个基本模型的预测结果作为输入，训练一个元模型（也称为组合模型）来进行最终的预测。与 Bagging 和 Boosting 不同，Stacking 不是简单地对基本模型的预测结果进行组合，而是使用这些结果作为新的特征输入到元模型中。

​	Stacking 的步骤：

- 使用多个不同的基本模型对数据进行训练，得到它们各自的预测结果。
- 将这些预测结果作为新的特征，构建一个新的数据集。
- 使用新的数据集训练一个元模型，该元模型用来进行最终的预测。

​	Stacking 通常需要在交叉验证框架下进行，以避免过拟合。

总的来说，Bagging 是一种并行的集成学习方法，通过对数据集进行有放回抽样来构建多个基学习器；而 Boosting 是一种串行的集成学习方法，通过迭代训练多个基学习器，并根据前一次训练的结果来调整样本权重，以逐步提高模型的性能。



### AdaBoost

AdaBoost（Adaptive Boosting）是一种集成学习方法，通过组合多个弱学习器来构建一个强大的分类器。它是由Yoav Freund和Robert Schapire于1996年提出的。

过程：

1. **初始化权重**：对每个训练样本赋予相等的权重。

2. **训练弱分类器**：使用训练数据集和权重训练一个弱分类器（例如决策树）。

3. **计算错误率**：计算弱分类器在训练数据集上的错误率。

4. **更新权重**：增加分类错误的样本的权重，减少分类正确的样本的权重。

5. **构建模型**：将当前的弱分类器与前面的弱分类器加权组合成一个强分类器。

6. **重复**：重复步骤2-5，直到达到指定的迭代次数或者达到一定的性能要求。

公式：

在AdaBoost算法中，每个样本都有一个对应的权重，初始时每个样本的权重相等，随着每轮迭代，样本的权重会根据分类器的表现进行更新。假设有N个训练样本，初始时每个样本的权重为$w_i = \frac{1}{N}$，那么第t个分类器的权重为$\alpha_t$，错误率为$\epsilon_t$。更新权重的公式如下：

1. 如果分类正确：$w_i^{(t+1)} = w_i^{(t)} \cdot e^{-\alpha_t}$

2. 如果分类错误：$w_i^{(t+1)} = w_i^{(t)} \cdot e^{\alpha_t}$

其中，$\alpha_t = \frac{1}{2} \ln \left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$

AdaBoost算法最终的强分类器的输出由所有弱分类器的加权组合而成。通常，弱分类器的权重与其错误率相关，错误率越低的弱分类器，其权重越大。



#### Adaboost的拟合目标是什么

​	Adaboost（Adaptive Boosting）是一种集成学习算法，旨在提高弱分类器的性能。其拟合目标是通过组合多个弱分类器来创建一个强分类器，以提高整体模型的性能。

​	具体来说，Adaboost 的拟合目标是不断调整数据集的权重，使得每个新的弱分类器都能够专注于之前分类错误的样本，从而提高整体模型的准确性。在每次迭代中，Adaboost都会根据上一次迭代中分类错误的样本的权重来训练一个新的弱分类器，然后将这个弱分类器与之前的分类器组合起来，形成一个更强大的模型。这个过程会一直进行，直到达到预先设定的迭代次数或者模型达到满意的性能水平为止。

​	因此，Adaboost 的拟合目标可以总结为：通过迭代地训练弱分类器，并将它们组合成一个强大的集成模型，以提高整体的分类性能。



### XGBoost

https://blog.csdn.net/weixin_44852067/article/details/130346159

XGBoost（eXtreme Gradient Boosting）是一种基于梯度提升算法的高效、灵活和可扩展的机器学习库。其实现过程可以大致分为以下几个步骤：

1. **初始化模型**：
   - 首先，初始化一个基础模型，通常是一个简单的模型，如一个只有一个节点的树（树桩），作为初始的强学习器。

2. **迭代训练**：
   - 依次迭代训练多个决策树模型，每次训练的目标是最小化损失函数，以减少前一个模型的残差。这里的损失函数通常是平方损失（用于回归问题）或者对数损失（用于分类问题）。
   - 在每次迭代中，XGBoost 会计算当前模型的梯度和二阶导数，然后利用梯度和二阶导数来构建新的树模型，以最小化损失函数。

3. **模型正则化**：
   - 在构建每棵树的过程中，XGBoost 还会引入正则化项，包括 L1 正则化和 L2 正则化，以控制模型的复杂度，防止过拟合。

4. **叶子节点权重优化**：
   - 对于每棵树的叶子节点，XGBoost 还会利用叶子节点的一阶和二阶梯度信息来优化叶子节点的权重，以进一步提高模型的性能。

5. **模型组合**：
   - 最后，将所有训练得到的决策树模型进行组合，得到最终的 XGBoost 模型。通常采用加权平均或者投票的方式来组合多个模型，以获得更稳健和准确的预测结果。

XGBoost 的实现过程涉及了许多优化和技巧，包括梯度和二阶导数的计算、模型的正则化、叶子节点权重的优化等，这些技术使得 XGBoost 在训练过程中更加高效和稳健，从而在实际应用中取得了良好的性能表现。



### LightGBM

https://zhuanlan.zhihu.com/p/627313748

LightGBM（Light Gradient Boosting Machine，以下简称LGBM）是一个基于梯度提升决策树（Gradient Boosted Decision Trees，GBDT）的高效、可扩展的机器学习算法，作为GBDT框架的算法的一员，并且作为XGB算法的后来者，LGBM非常好综合了包括XGB在内的此前GBDT算法框架内各算法的一系列优势，并在此基础上做了一系列更进一步的优化。LGBM算法提出的核心目的是为了解决GBDT算法框架在处理海量数据时计算效率低下的问题，而从实践效果来看，LGBM也确实做到了这点——LGBM以牺牲极小的计算精度为代价，将GBDT的计算效率提升了近20倍！这也最终使得LGBM算法是第一个真正意义上能处理海量数据的GBDT框架算法。并且，尽管计算精度存在“选择性的牺牲”，但LGBM的实际建模效果也能达到几乎和XGB同等水平，而且由于LGBM“选择性的牺牲精度”从另一个角度来看其实就是抑制模型过拟合，因此在很多场景下，LGBM的算法效果甚至会好于XGB。种种实践证明，LGBM是一个拥有超高计算效率的同时、又能够保持超高精度的算法，是目前机器学习领域当之无愧的顶级算法之一。

而LGBM是如何做效率和精度“两手抓”的呢？简而言之就是LGBM充分借鉴了XGB提出的一系列提升精度的优化策略，同时在此基础之上进一步提出了一系列的**数据压缩**和**决策树建模流程的优化策略**。尽管在算法的数学原理层面LGBM并没有翻越XGB创建的理论高峰，但其提出的一系列优化策略也同样是极具开创性的，其中**数据压缩方法**能够让实际训练的数据量在大幅压缩的同时仍然保持较为完整的信息，而**决策树建模流程方面的优化**，则是在XGB提出的直方图优化算法基础上进行了大幅优化，不仅能够加速决策树建模速度，同时也能非常好的处理经过压缩后的数据，从而最终大幅提升每棵树的训练效率（甚至在LGBM提出的一段时间后，新版XGB也采用了LGBM类似的直方图算法来加速建模效率）。并且最重要的是，有理论能够证明，哪怕LGBM实际建模是基于压缩后的数据进行训练，但其预测精度受到的影响也是微乎其微。

当然，除了算法原理层面的优化方法外，LGBM还提出了非常多针对于实际计算过程的优化，例如Voting Parallel（投票特征并行）方法、特征多线程并行处理方法、GPU加速方法和分布式计算等，这些方法进一步提升了LGBM实际建模效率，并且一定程度拓宽了算法的使用场景。并且需要注意的是，所谓的计算效率优化，不仅体现在计算时间的大幅缩短，同时得益于LGBM所提出的一系列数据压缩技术，使得实际建模时数据内存占用也大幅减少。

总的来说，LGBM算法可以看成是迭代过程几乎全盘借鉴XGB、而在**数据压缩方法**和**决策树训练方法**上有大量创新的算法。

LightGBM建模过程总共会进行三方面的数据压缩：

1. 连续变量分箱——降低分裂点的数量
2. 互斥特征捆绑（Exclusive Feature Bundling, EFB）——降低特征数量
3. 基于梯度的单边采样（Gradient-based One-Side Sampling, GOSS）——降低样本数量

根据实际建模顺序，会现在全样本上连续变量分箱（连续变量离散化），然后同时带入离散特征和离散后的连续变量进行离散特征捆绑（合并）降维，最终在每次构建一颗树之前进行样本下采样。

其中连续变量的分箱就是非常简单的等宽分箱，并且具体箱体的数量可以通过超参数进行人工调节；而离散特征的降维，则是采用了一种所谓的互斥特征捆绑（Exclusive Feature Bundling, EFB）算法，该算法也是由LGBM首次提出，该方法的灵感来源于独热编码的逆向过程，通过把互斥的特征捆绑到一起来实现降维，这种方法能够很好的克服传统降维方法带来的信息大量损耗的问题，并且需要注意的是，输入EFB进行降维的特征，即包括原始离散特征，也包括第一阶段连续变量离散化之后的特征；在这一系列数据压缩之后，LGBM在每次迭代（也就是每次训练一颗决策树模型）的时候，还会围绕训练数据集进行下采样，此时的下采样不是简单的随机抽样，而是一种名为基于梯度的单边采样（Gradient-based One-Side Sampling, GOSS）的方法，和EFB类似，这种方法能够大幅压缩数据，但同时又不会导致信息的大量损失。不难发现，最终输入到每颗决策树进行训练的数据，实际上是经过大幅压缩后的数据，这也是LGBM计算高效的根本原因之一。



### GBDT

https://blog.csdn.net/m0_56006701/article/details/130832837

AdaBoost每一轮基学习器训练过后都会更新样本权重，再训练下一个学习器，最后将所有的基学习器加权组合。AdaBoost使用的是指数损失，这个损失函数的缺点是对于异常点非常敏感，因而通常在噪音比较多的数据集上表现不佳。Gradient Boosting在这方面进行了改进，使得可以使用任何损失函数 (只要损失函数是连续可导的)，这样一些比较robust的损失函数就能得以应用，使模型抗噪音能力更强。
和Adaboost不同，Gradient Boosting 在迭代的时候选择梯度下降的方向来保证最后的结果最好。
算法将负梯度作为残差值来学习基本模型h(x).



## LDA

LDA（Latent Dirichlet Allocation）是一种用于文本数据的概率主题模型。它的基本思想是假设文档是由多个主题混合而成的，每个主题又由多个单词组成。在LDA中，每个文档可以包含多个主题，而每个主题又可以包含多个单词，但每个单词只能属于一个主题。

LDA的原理可以简单描述如下：

1. **初始化**：首先，对文档集中的每个文档，随机分配一些主题，然后对文档中的每个单词，随机分配一个主题。

2. **迭代**：在每次迭代中，对于每个文档中的每个单词，计算其分配给每个主题的概率。这个概率取决于该单词在当前主题中的频率以及当前文档中该主题的频率。

3. **重分配**：根据计算得到的概率重新分配每个单词的主题。

4. **迭代收敛**：重复上述步骤直到模型收敛，即直到主题分配不再发生显著变化为止。

在LDA模型收敛之后，每个单词都会被分配到一个主题上，而每个文档则会被表示为各个主题的分布。通过这种方式，LDA可以用于发现文档集中的潜在主题，并且可以根据需要对文档进行主题分类或者对单词进行主题分类。

总的来说，LDA是一种无监督学习方法，通过对文档集进行概率建模，从而发现文档集中的主题结构。



## 梯度提升算法

梯度提升算法（Gradient Boosting）是一种集成学习方法，主要用于解决回归和分类问题。它通过迭代地训练一系列的弱学习器（通常是决策树），并将它们组合成一个强学习器，以提高模型的预测性能。

具体来说，梯度提升算法的工作流程如下：

1. **初始化模型**：
   - 首先，初始化一个简单的模型作为初始的强学习器，通常选择一个简单的模型，如一个只有一个节点的树（即树桩）。

2. **迭代训练**：
   - 依次迭代训练多个弱学习器，每个弱学习器都在前一个模型的残差上进行训练，目标是减少前一个模型的残差，逐步提高模型的预测性能。
   - 在每次迭代中，通过梯度下降法来优化损失函数，使得模型在训练数据上的预测误差逐步减小。

3. **模型组合**：
   - 将所有训练得到的弱学习器进行组合，得到最终的强学习器。通常采用加权平均或者投票的方式来组合多个模型，以获得更稳健和准确的预测结果。

梯度提升算法的主要思想是通过反复迭代地训练弱学习器，并利用梯度下降法来逐步减少模型的预测误差，从而得到一个性能较好的强学习器。相比于单一的模型，梯度提升算法通常能够提高模型的预测准确性，并且具有较强的泛化能力，因此在实际应用中得到了广泛的应用。



